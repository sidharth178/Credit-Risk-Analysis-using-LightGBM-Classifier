# -*- coding: utf-8 -*-
"""HACKANONS COLAB 25GB RAM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GqZu6zmCy2vMNMZqO78DDuHvwv_9vJcp

# **1. Overview**

### **Context :** 
Banks need to protect their interest before it can take risk on you and issue credit card to you. Banks use their previous credit card holders records for understanding the patterns of the card holders. It is a lot more complex process to predict whether a person who they do not know at personal level, will be a defaulter or not. Banks, along with the data from their own records, also use CIBIL data.
Based on all this data, banks want to develop a pattern that will tell them who are likely to be a defaulter and who are not.

### **Attribute Information :** 
The dataset has 13 features with 50636 observations. The features are:

Here <b>age, gender</b> are the age and gender of the card holder.<br>
<b>education</b> is the last acquired educational qualification of the card holder.<br>
<b>occupation</b> can be salaried, or self employed or business etc. <br>
<b>organization_type</b> can be tire 1, 2, 3 etc.<br> 
<b>seniority</b> denotes at which career level the card holder is in.<br> 
<b>annual_income</b> is the gross annual income of the card holder.<br> 
<b>disposable_income</b> is annual income - recurring expenses.<br> 
<b>house_type</b> is owned or rented or company provided etc.<br> 
<b>vehicle_type</b> is 4-wheeler or two-wheeler or none.<br> 
<b>marital_status</b> is of the card holder.<br> 
<b>no_card</b> has the information of the number of other credit cards that the card holder already holds. <br>
And at the end of each row, we have a <b>defaulter</b> indicator indicating whether the card holder was a defaulter or not. It is 1 if the card holder was a defaulter, 0 otherwise.
### **Objective :**
We have to use this dataset to generate a classification model that can successfully predict for a new applicant with recorded data for given parameters in the data set, if he is likely to be a defaulter.

# **2. Import Libraries:**
"""

# import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

import xgboost as xgb
import lightgbm as lgb
from sklearn.svm import SVC
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.model_selection import RandomizedSearchCV,GridSearchCV
from sklearn.model_selection import KFold
from datetime import datetime
import warnings

"""# **3. Load Dataset:**"""

# Load the dataset
credit_df=pd.read_csv("credit_data.csv")
credit_df.head()

"""# **4. Exploratory Data Analysis**"""

credit_df.shape

credit_df.info()

# check for null values
credit_df.isnull().sum()

# see the statistical analysis of the dataset
credit_df.describe()

# Separate columns having categorical value
obj_df=credit_df.select_dtypes(include=['object']).copy()
obj_df.head()

# find unique values present in "no_card" column
credit_df['no_card'].value_counts()

"""Here we saw that column **'no_card'** has categorical values but the values are integer. So we'd add that column to the **obj_df** dataframe."""

# add column "no_card" to the dataframe "obj_df"
obj_df['no_card']=credit_df['no_card']

obj_df.head()

obj_df.columns

# Find number of unique value present in all columns
obj_df.nunique()

def find_unique(col):
  print(col,":",obj_df[col].unique())
for col in obj_df.columns:
  find_unique(col)

"""# **5. Data Visualization**

**Frequency distribution of columns having categorical values**
"""

def plot_bar_graph(column_name):
    ed_count = column_name.value_counts()
    sns.set(style="darkgrid")
    sns.barplot(ed_count.index, ed_count.values, alpha=0.9)
    plt.title('Frequency Distribution of {} Levels using Bar Plot'.format(column_name.name))
    plt.ylabel('Number of Occurrences', fontsize=12)
    plt.xlabel('{}'.format(column_name.name), fontsize=12)
    plt.show()

for i in obj_df.columns:
  plot_bar_graph(obj_df[i])

numeric_df=credit_df.select_dtypes(include=['int64']).copy()
numeric_df.drop(columns=['no_card','default'],axis=1,inplace=True)
numeric_df.head()

def distplot_graph(column_name):
    sns.set(style="darkgrid")
    sns.distplot(numeric_df[column_name], color="g")
    plt.title('Normal Distribution of {} Levels using Bar Plot'.format(column_name))    
    plt.xlabel('{}'.format(column_name), fontsize=12)
    plt.show() 
    
for col in numeric_df.columns:
  distplot_graph(col)

"""# **6. Data Preprocessing**"""

# encode the categorical values into numeric values
le=LabelEncoder()
for col in obj_df.columns:
    credit_df[col]=le.fit_transform(credit_df[col])
credit_df.head()

"""### **6.1. Outlier Detection**"""

# find outlier in all columns
for i in credit_df.select_dtypes(include=['float64','int64']).columns:
  max_thresold=credit_df[i].quantile(0.95)
  min_thresold=credit_df[i].quantile(0.05)
  credit_df_no_outlier=credit_df[(credit_df[i] < max_thresold) & (credit_df[i] > min_thresold)].shape
  print(" outlier in ",i,"is" ,int(((credit_df.shape[0]-credit_df_no_outlier[0])/credit_df.shape[0])*100),"%")

# remove outliers from columns having nearly 10% outlier
max_thresold_annual_income=credit_df["annual_income"].quantile(0.95)
min_thresold_annual_income=credit_df["annual_income"].quantile(0.05)
max_thresold_disposable_income=credit_df["disposable_income"].quantile(0.95)
min_thresold_disposable_income=credit_df["disposable_income"].quantile(0.05)
credit_df_no_outlier=credit_df[(credit_df["annual_income"] < max_thresold_annual_income) & (credit_df["annual_income"] > min_thresold_annual_income) &  (credit_df["disposable_income"] < max_thresold_disposable_income) & (credit_df["disposable_income"] > min_thresold_disposable_income)]

credit_df_no_outlier.head()

credit_df_no_outlier.reset_index(inplace=True)
credit_df_no_outlier.head()

credit_df_no_outlier=credit_df_no_outlier.drop(columns=["index"],axis=1)
credit_df_no_outlier.head()

"""### **6.2. Checking Data Correlation**"""

# correlation matrix of DataFrame
plt.figure(figsize=(20,10))
corn=credit_df_no_outlier.corr()
sns.heatmap(corn,annot=True,cmap="BuPu")

X=credit_df_no_outlier.drop(columns=["default"])
y=credit_df_no_outlier["default"]

# scale the DataFrame
scalar=StandardScaler()
X=scalar.fit_transform(X)
X=pd.DataFrame(X)

"""# **7. Split the dataset**

Here before going for <code>hyper-parameter</code> tuning we'll apply <code>LazyClassifier</code> algorithm to find which algorithm will give how much of accuracy.
"""

x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=21)

print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)

"""# **8. Apply Lazypredict**

As the dataset is too big when we apply <code>LazyClassifier</code> algorithm our execution may crash due to less RAM. Google colab provides 12gb RAM for free but to execute this algorithm with big dataset we need more RAM. So i'm using Google colab pro to execute this algorithm. Don't worry if you have not Colab pro. I'll provide the output in a csv file.
"""

!pip install lazypredict==0.2.7

!pip install lightgbm

import lazypredict
from lazypredict.Supervised import LazyClassifier

clf= LazyClassifier(verbose=0,ignore_warnings=True,custom_metric=None)
train,test=clf.fit(x_train,x_test,y_train,y_test)

train

"""NOTE: If the execution got failed or crashed then run the below cell to see the output of upper cells and make comment the above five cells"""

#  all_algorithm_df=pd.read_csv("lazypredict_algo.csv")
#  all_algorithm_df

"""# **9. Hyperparameter Tuning**"""

# this code is to show how much time required to train the model using different algorithms
def timer(start_time= None):
  if not start_time:
    start_time=datetime.now()
    return start_time
  elif start_time:
    thour,temp_sec=divmod((datetime.now()-start_time).total_seconds(),3600)
    tmin,tsec=divmod(temp_sec,60)
    print('\n Time taken: %i hours %i minutes and %s seconds. '% (thour,tmin,round(tsec,2)))

# parameters of all classification algorithms respectively
model_param={    
    'random_forest':{
        'model':RandomForestClassifier(),
        'params':{
            'n_estimators':[100,1000]
        }
    },
    'LGBMClassifier':{
        'model':lgb.LGBMClassifier(),
        'params':{
            'max_depth': [20, 30],
            'max_bin':[255,300],                      
            'n_estimators' : [10,100,1000]
        }
    },
    'ExtraTreesClfr':{
        'model':ExtraTreesClassifier(),
        'params':{
            "max_depth": [None],
            "max_features": [8,12],
            "min_samples_split": [10,20,30],
            "bootstrap": [False, True],
            "n_estimators" :[10,100,200],
            "criterion": ["gini"]
        }
    }
}

"""This below cell may take a little more time to execute."""

start_time=timer(None)
scores=[]
for model_name,mp in model_param.items():
    # Apply GridSearchCV
    rs=GridSearchCV(mp['model'],mp['params'],cv=5,return_train_score=False)
    rs.fit(X,y)
    print( model_name," : successfully execute")
    scores.append({
        'model':model_name, # it'll retrive the best model name
        "best_score":rs.best_score_, # it'll retrive the best accuracy score
        'best_params':rs.best_params_ # it'll retrive the best parameter
    })
timer(start_time)

df=pd.DataFrame(scores,columns=['model','best_score','best_params'])
df

# Find the parameter of LGBMClassifier
# rs.best_params_
# rs.best_estimator_
scores[1]

"""Now we are going to split the dataset into two parts where 90% of data will be use in KFold Cross validation and rest 10% data will be use for sample prediction """

# Split the dataset
x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.10,random_state=21)

# reset the index of x_train
x_train.reset_index(inplace=True)
# drop "index" column
x_train.drop(columns=['index'],axis=1,inplace=True)
x_train.tail()

y_train=pd.DataFrame(y_train)
y_train.reset_index(inplace=True)
y_train.drop(columns=['index'],axis=1,inplace=True)
y_train.tail()

# convert "y_train" dataframe into an array 
y_train=np.array(y_train)

"""# **10. Model Building**"""

# Hide warning
warnings.filterwarnings('ignore')

# Apply KFold
skf=KFold(n_splits=5)
LGBMClf= lgb.LGBMClassifier(max_bin= 300, max_depth = 20, n_estimators = 100)

for train_index,test_index in skf.split(x_train,y_train):
    x_train_kf,x_test_kf,y_train_kf,y_test_kf = x_train.loc[train_index],x_train.loc[test_index],y_train[train_index],y_train[test_index]
    LGBMClf.fit(x_train_kf, y_train_kf)

# Find Accuracy
print("============================================================")
print("Training accuracy of data is :", LGBMClf.score(x_train_kf, y_train_kf))
print("============================================================")
print("Testng accuracy of data is :", LGBMClf.score(x_test_kf, y_test_kf))
print("============================================================")

"""# **11. Save the Model**"""

#Saving Scikitlearn models
import joblib
joblib.dump(LGBMClf, "credit_risk_analyzer.pkl")

# load the model
model=joblib.load("credit_risk_analyzer.pkl")
model

"""# **12. Find Prediction**"""

y_pred=model.predict(x_test)